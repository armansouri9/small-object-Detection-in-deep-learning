{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SN4rjJYHCBP",
        "outputId": "20b961bc-d239-40a7-dff6-1ecf7e692a97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDPAdZMTIEK5"
      },
      "outputs": [],
      "source": [
        "#!mkdir data\n",
        "#!unrar x /content/drive/MyDrive/birds_dataset_train.rar /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "305cSJ98Jifk"
      },
      "outputs": [],
      "source": [
        "#!mkdir data\n",
        "#!unrar x /content/drive/MyDrive/birds_dataset_validation.rar /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwXEdZOO3j2w"
      },
      "outputs": [],
      "source": [
        "#!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5S0exe33N8u6",
        "outputId": "d5adb5ee-3a04-4f66-d35e-daf6106543d3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import glob\n",
        "from PIL import Image\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import functional as F\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from matplotlib.pyplot import figure\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l43n-SbREMxs",
        "outputId": "d3489431-465c-4f13-a699-73833d539a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/drive/MyDrive/*.png': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm /content/drive/MyDrive/*.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfFNar1kTeRE"
      },
      "outputs": [],
      "source": [
        "def yolo_to_pascal_voc(x_center, y_center, w, h,  image_w, image_h):\n",
        "    w = w * image_w\n",
        "    h = h * image_h\n",
        "    x1 = ((2 * x_center * image_w) - w)/2\n",
        "    y1 = ((2 * y_center * image_h) - h)/2\n",
        "    x2 = x1 + w\n",
        "    y2 = y1 + h\n",
        "    return [x1, y1, x2, y2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjmiaxZoy3Uh"
      },
      "outputs": [],
      "source": [
        "train_limit=500\n",
        "valid_limit=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5i2yr7pTBZ6"
      },
      "outputs": [],
      "source": [
        "!rm /content/drive/MyDrive/rcnn_dataset/data/train/train_labels.csv\n",
        "!rm /content/drive/MyDrive/rcnn_dataset/data/validation/validation_labels.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq30rJeYN5Ba"
      },
      "outputs": [],
      "source": [
        "os.chdir(r'/content/drive/MyDrive/rcnn_dataset/data/train/annotations')\n",
        "myFiles = glob.glob('*.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebjFg--QKD1L"
      },
      "outputs": [],
      "source": [
        "final_df=[]\n",
        "k=0\n",
        "for item in myFiles:\n",
        "    k+=1\n",
        "    if k>=train_limit:\n",
        "      break\n",
        "    try:\n",
        "      img=Image.open(\"/content/drive/MyDrive/rcnn_dataset/data/train/sequences/\"+item[0:-3]+'jpg')  \n",
        "      with open(item, 'rt') as fd:\n",
        "          for first_line in fd.readlines():\n",
        "            row=[]\n",
        "            bbox_temp=[]\n",
        "            splited = first_line.split();\n",
        "            row.append(item[0:-3]+'jpg')\n",
        "            try:\n",
        "                x1,y1,x2,y2=yolo_to_pascal_voc(float(splited[1]),float(splited[2]),float(splited[3]),float(splited[4]),img.size[0],img.size[1])\n",
        "                bbox_temp.append(float(x1))\n",
        "                bbox_temp.append(float(y1))\n",
        "                bbox_temp.append(float(x2))\n",
        "                bbox_temp.append(float(y2))\n",
        "                row.append(bbox_temp)\n",
        "                final_df.append(row)\n",
        "                \n",
        "            except:\n",
        "                print(\"file is not in YOLO format!\")\n",
        "    except:\n",
        "      pass \n",
        "df = pd.DataFrame(final_df,columns=['image','bbox'])\n",
        "df.to_csv(\"/content/drive/MyDrive/rcnn_dataset/data/train/train_labels.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRC0MJmERsdB"
      },
      "outputs": [],
      "source": [
        "os.chdir(r'/content/drive/MyDrive/rcnn_dataset/data/validation/annotations')\n",
        "myFiles = glob.glob('*.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ndbi7Wd_Rutn"
      },
      "outputs": [],
      "source": [
        "final_df=[]\n",
        "k=0\n",
        "for item in myFiles:\n",
        "    k+=1\n",
        "    if k>=valid_limit:\n",
        "      break\n",
        "    try:\n",
        "      img=Image.open(\"/content/drive/MyDrive/rcnn_dataset/data/validation/sequences/\"+item[0:-3]+'jpg') \n",
        "      with open(item, 'rt') as fd:\n",
        "          for first_line in fd.readlines():\n",
        "            row=[]\n",
        "            bbox_temp=[]\n",
        "            splited = first_line.split();\n",
        "\n",
        "            row.append(item[0:-3]+'jpg')\n",
        "            try:\n",
        "                x1,y1,x2,y2=yolo_to_pascal_voc(float(splited[1]),float(splited[2]),float(splited[3]),float(splited[4]),img.size[0],img.size[1])\n",
        "                bbox_temp.append(float(x1))\n",
        "                bbox_temp.append(float(y1))\n",
        "                bbox_temp.append(float(x2))\n",
        "                bbox_temp.append(float(y2))\n",
        "                row.append(bbox_temp)\n",
        "                final_df.append(row)\n",
        "            except:\n",
        "                print(\"file is not in YOLO format!\")\n",
        "    except:\n",
        "      pass\n",
        "df = pd.DataFrame(final_df,columns=['image','bbox'])\n",
        "df.to_csv(\"/content/drive/MyDrive/rcnn_dataset/data/validation/validation_labels.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6j5LmxLWM56"
      },
      "outputs": [],
      "source": [
        "class BirdsDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, root, phase):\n",
        "    self.root = root\n",
        "    self.phase = phase\n",
        "\n",
        "    self.targets = pd.read_csv(os.path.join(root, '{}_labels.csv'.format(phase)))\n",
        "    self.imgs = list(dict.fromkeys(self.targets['image']))\n",
        "    \n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.root, 'sequences', self.imgs[idx])\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img = F.to_tensor(img)\n",
        "    #\n",
        "    box_list = self.targets[self.targets['image'] == self.imgs[idx]]\n",
        "\n",
        "    \n",
        "    floats = [eval(x) for x in box_list['bbox'].values]\n",
        "    boxes =torch.tensor(floats, dtype=torch.float32)\n",
        "    \n",
        "\n",
        "    #\n",
        "    labels = torch.ones((len(box_list), ), dtype=torch.int64)\n",
        "    #\n",
        "    target = {}\n",
        "    target['boxes'] = boxes\n",
        "    target['labels'] = labels\n",
        "    #\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP3u3ULMXqQN",
        "outputId": "c307c431-254c-4457-fb35-2fd7e833d6bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train len : 499\n",
            "Valid len : 9\n"
          ]
        }
      ],
      "source": [
        "train_dataset = BirdsDataset('/content/drive/MyDrive/rcnn_dataset/data/train', 'train')\n",
        "\n",
        "valid_dataset = BirdsDataset('/content/drive/MyDrive/rcnn_dataset/data/validation', 'validation')\n",
        "\n",
        "print(\"train len :\",train_dataset.__len__())\n",
        "print(\"Valid len :\",valid_dataset.__len__())\n",
        "\n",
        "#print(train_dataset.__getitem__(1))\n",
        "#print(valid_dataset.__getitem__(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hvd6FnyMiwW1"
      },
      "outputs": [],
      "source": [
        "def new_concat(batch):\n",
        "  return tuple(zip(*batch))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                            batch_size=1,\n",
        "                            shuffle=True,\n",
        "                            collate_fn=new_concat)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                            batch_size=1,\n",
        "                            shuffle=True,\n",
        "                            collate_fn=new_concat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iisMuB5Xi2nO",
        "outputId": "0c9b74d2-df1f-48f8-9d65-c7a557c070ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((tensor([[[0.1020, 0.1020, 0.1020,  ..., 0.1765, 0.1765, 0.1765],\n",
              "           [0.1020, 0.1020, 0.1020,  ..., 0.1765, 0.1765, 0.1765],\n",
              "           [0.1020, 0.1020, 0.1020,  ..., 0.1765, 0.1765, 0.1765],\n",
              "           ...,\n",
              "           [0.0980, 0.0941, 0.0941,  ..., 0.0980, 0.0980, 0.0980],\n",
              "           [0.0980, 0.0941, 0.0941,  ..., 0.0980, 0.0980, 0.0980],\n",
              "           [0.0980, 0.0941, 0.0941,  ..., 0.0980, 0.0980, 0.0980]],\n",
              "  \n",
              "          [[0.1137, 0.1137, 0.1137,  ..., 0.1608, 0.1608, 0.1608],\n",
              "           [0.1137, 0.1137, 0.1137,  ..., 0.1608, 0.1608, 0.1608],\n",
              "           [0.1137, 0.1137, 0.1137,  ..., 0.1608, 0.1608, 0.1608],\n",
              "           ...,\n",
              "           [0.1451, 0.1412, 0.1412,  ..., 0.1412, 0.1412, 0.1412],\n",
              "           [0.1451, 0.1412, 0.1412,  ..., 0.1412, 0.1412, 0.1412],\n",
              "           [0.1451, 0.1412, 0.1412,  ..., 0.1412, 0.1412, 0.1412]],\n",
              "  \n",
              "          [[0.0706, 0.0706, 0.0706,  ..., 0.1176, 0.1176, 0.1176],\n",
              "           [0.0706, 0.0706, 0.0706,  ..., 0.1176, 0.1176, 0.1176],\n",
              "           [0.0706, 0.0706, 0.0706,  ..., 0.1176, 0.1176, 0.1176],\n",
              "           ...,\n",
              "           [0.0980, 0.0941, 0.0941,  ..., 0.1098, 0.1098, 0.1098],\n",
              "           [0.0980, 0.0941, 0.0941,  ..., 0.1098, 0.1098, 0.1098],\n",
              "           [0.0980, 0.0941, 0.0941,  ..., 0.1098, 0.1098, 0.1098]]]),),\n",
              " ({'boxes': tensor([[1134.9005,  752.3202, 1230.7008,  873.6006],\n",
              "           [ 635.1907,  846.2208,  674.0611,  877.4904],\n",
              "           [ 931.6099,  768.0000,  968.7600,  803.2200],\n",
              "           [ 378.5693, 1044.3396,  429.3398, 1097.2500],\n",
              "           [ 423.1306,  741.6900,  470.0304,  786.6396],\n",
              "           [ 316.7299,  899.6604,  339.7603,  932.5404],\n",
              "           [ 429.7699,  881.8194,  457.8499,  916.9998],\n",
              "           [ 330.5894, 1046.5200,  369.2697, 1086.2496],\n",
              "           [ 245.8589,  849.6804,  281.1696,  878.3700],\n",
              "           [1153.7904,  207.0300, 1178.0995,  268.8996],\n",
              "           [1160.8599,   84.9006, 1185.0999,  134.4306],\n",
              "           [1193.9607,   88.0794, 1220.4010,  137.5998],\n",
              "           [1096.8989,  262.4298, 1112.9098,  278.3094],\n",
              "           [ 560.5603,  835.0602,  610.5399,  868.8102],\n",
              "           [ 986.5306,  694.9698, 1002.1401,  710.7798],\n",
              "           [1152.4810,    1.9200, 1177.8806,   35.1504],\n",
              "           [1369.1002,  466.8702, 1388.9607,  488.3706]]),\n",
              "   'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])},))"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "389dfcb346e146359d9724c907c43e0a",
            "d7c35cc1a11c4a02bd8540beb1e64deb",
            "4ef9d029058d412d8a60092c02f12b38",
            "08b86d585d0e4f04af49c761647d24a6",
            "ebb74caa5ae54fa8b671e11ab389566f",
            "e254392fc242426ca953e846e9950f60",
            "5540a821425b42c684a6b81f40b31665",
            "0172c15cbcfd410b8312f74ae56fa49f",
            "090bd994095641dca6ecad666d59c138",
            "3482e6827d374d52bb2c78edf62109ad",
            "618b48cbed3545b89c7e4e1a8bf556b7"
          ]
        },
        "id": "fmzvjnMUjFpu",
        "outputId": "67420649-393a-4878-ec42-4a9eda9772a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "389dfcb346e146359d9724c907c43e0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "#backbone=torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "#modules = list(backbone.children())[0:1]\n",
        "#backbone = torch.nn.Sequential(*modules)\n",
        "\n",
        "#backbone.out_channels=1280\n",
        "\n",
        "\n",
        "'''anchor_generator = AnchorGenerator(\n",
        "        sizes=((32, 64, 128, 256, 512),),\n",
        "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
        "    )'''\n",
        "\n",
        "    # Feature maps to perform RoI cropping.\n",
        "    # If backbone returns a Tensor, `featmap_names` is expected to\n",
        "    # be [0]. We can choose which feature maps to use.\n",
        "'''roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "        featmap_names=['0'],\n",
        "        output_size=7,\n",
        "        sampling_ratio=2\n",
        "    )'''\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "#model = torchvision.models.detection.FasterRCNN(backbone=backbone,num_classes=2,rpn_anchor_generator=anchor_generator,\n",
        "#        box_roi_pool=roi_pooler)\n",
        "\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(1024, 2)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "M2ah0p46YpDw",
        "outputId": "42bb1851-002a-43b0-960a-b980f43f2f2f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'import torchvision\\nbackbone=torchvision.models.mobilenet_v2(pretrained=True).features\\nmodules = list(backbone.children())[0:3]  # delete the last fc layer.\\n#backbone_nn = nn.Sequential(*modules)\\nmodules'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''import torchvision\n",
        "backbone=torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "modules = list(backbone.children())[0:3]  # delete the last fc layer.\n",
        "#backbone_nn = nn.Sequential(*modules)\n",
        "modules'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztv9y7Z3jVea"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "metric = MeanAveragePrecision(iou_type=\"bbox\",iou_thresholds=[0.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElWDX3LkjX2p"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def train_one_epoch(model, optimizer, train_dataloader,valid_dataloader):\n",
        "    \n",
        "    model.train()\n",
        "    train_total_loss = 0\n",
        "    for images, targets in train_dataloader:\n",
        "        images = [image.to(device) for image in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        train_total_loss += losses\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    valid_total_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for images, targets in valid_dataloader:\n",
        "          images = [image.to(device) for image in images]\n",
        "          targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "          loss_dict = model(images, targets)\n",
        "          losses = sum(loss for loss in loss_dict.values())\n",
        "          valid_total_loss += losses\n",
        "    \n",
        "    '''model.eval()    \n",
        "    train_maps = 0\n",
        "    with torch.no_grad():\n",
        "      for images, targets in train_dataloader:\n",
        "         images = [image.to(device) for image in images]\n",
        "         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "         pred = model(images, targets)\n",
        "         metric.update(pred, targets)\n",
        "         map=metric.compute()['map'].item()\n",
        "         train_maps+=map\n",
        "\n",
        "    val_maps = 0\n",
        "    with torch.no_grad():\n",
        "      for images, targets in valid_dataloader:\n",
        "         images = [image.to(device) for image in images]\n",
        "         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "         pred = model(images, targets)\n",
        "         metric.update(pred, targets)\n",
        "         map=metric.compute()['map'].item()\n",
        "         val_maps+=map'''\n",
        "\n",
        "    return train_total_loss/len(train_dataloader),valid_total_loss/len(valid_dataloader)#,train_maps/len(train_dataloader),val_maps/len(valid_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtMV_8dbjade",
        "outputId": "4fca5af3-de2b-4075-94ec-28bcc0875bca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [02:51<25:40, 171.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " epoch [0]:  \t lr: [0.005]  \n",
            "train_loss: 0.9445655345916748 \t valid_loss: 1.403616189956665 \t \n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [05:38<22:32, 169.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " epoch [1]:  \t lr: [0.005]  \n",
            "train_loss: 0.5856483578681946 \t valid_loss: 1.6152652502059937 \t \n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [08:26<19:40, 168.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " epoch [2]:  \t lr: [0.005]  \n",
            "train_loss: 0.4860480725765228 \t valid_loss: 2.5422205924987793 \t \n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [11:14<16:49, 168.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " epoch [3]:  \t lr: [0.0005]  \n",
            "train_loss: 0.3534965217113495 \t valid_loss: 2.474836587905884 \t \n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [14:02<14:00, 168.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " epoch [4]:  \t lr: [0.0005]  \n",
            "train_loss: 0.33042678236961365 \t valid_loss: 2.764439344406128 \t \n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [16:49<11:11, 167.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " epoch [5]:  \t lr: [0.0005]  \n",
            "train_loss: 0.3190670311450958 \t valid_loss: 2.6036949157714844 \t \n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [19:38<08:24, 168.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " epoch [6]:  \t lr: [5e-05]  \n",
            "train_loss: 0.3062446117401123 \t valid_loss: 2.6718690395355225 \t \n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [22:26<05:35, 168.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " epoch [7]:  \t lr: [5e-05]  \n",
            "train_loss: 0.3039996325969696 \t valid_loss: 2.7108922004699707 \t \n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [25:13<02:47, 167.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " epoch [8]:  \t lr: [5e-05]  \n",
            "train_loss: 0.3022426664829254 \t valid_loss: 2.7427611351013184 \t \n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [28:00<00:00, 168.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " epoch [9]:  \t lr: [5e-06]  \n",
            "train_loss: 0.3022397756576538 \t valid_loss: 2.6858341693878174 \t \n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_loss_values=[]\n",
        "valid_loss_values=[]\n",
        "train_mAPs=[]\n",
        "val_mAPs=[]\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    #train_loss,valid_loss,train_map,val_map = train_one_epoch(model, optimizer, train_loader,valid_loader)\n",
        "    train_loss,valid_loss = train_one_epoch(model, optimizer, train_loader,valid_loader)\n",
        "\n",
        "    train_loss_values.append(train_loss.item())\n",
        "    valid_loss_values.append(valid_loss.item())\n",
        "    #train_mAPs.append(train_map)\n",
        "    #val_mAPs.append(val_map)\n",
        "    print(' epoch [{}]:  \\t lr: {}  \\ntrain_loss: {} \\t valid_loss: {} \\t \\n\\n'.format(epoch, lr_scheduler.get_last_lr(), train_loss,valid_loss))\n",
        "\n",
        "    #print(' epoch [{}]:  \\t lr: {}  \\ntrain_loss: {} \\t valid_loss: {} \\t train_mAP:{} \\t val_mAP:{} \\n\\n'.format(epoch, lr_scheduler.get_last_lr(), train_loss,valid_loss,train_map,val_map))\n",
        "    lr_scheduler.step()\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(),'/content/drive/MyDrive/model/model.pt') # Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDq4ruroEC5h",
        "outputId": "cecc02fb-a0df-49f8-8ce8-7a6e97703e64"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(num_epochs), train_loss_values, '-b', label='train_loss')\n",
        "plt.plot(range(num_epochs), valid_loss_values, '-r', label='valid_loss')\n",
        "\n",
        "plt.xlabel(\"n iteration\")\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Model Loss')\n",
        "plt.savefig('/content/drive/MyDrive/model/loss.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "#plt.plot(range(num_epochs), train_mAPs, '-b', label='train_mAP')\n",
        "#plt.plot(range(num_epochs), val_mAPs, '-r', label='val_mAP')\n",
        "#plt.xlabel(\"n iteration\")\n",
        "\n",
        "#plt.legend(loc='upper left')\n",
        "#plt.title('Model mAP')\n",
        "#plt.savefig('/content/drive/MyDrive/model/mAP.png', bbox_inches='tight')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0uYy-vxWUlI",
        "outputId": "28da750d-0444-4791-c097-0a4cf13704b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model/model.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K0Cu3J7_z7dV"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_dataloader):\n",
        "    evaluate_maps=0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        cnt = 0\n",
        "        for images , targets in test_dataloader:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            out = model(images)\n",
        "            scores = out[0]['scores'].cpu().numpy()\n",
        "            inds = scores > 0.7\n",
        "            bxs = out[0]['boxes'].cpu().numpy()\n",
        "            bxs = bxs[inds]\n",
        "            gt = targets[0]['boxes'].cpu().numpy()\n",
        "            # gt = gt[0]\n",
        "            img = images[0].permute(1, 2, 0).cpu().numpy()\n",
        "            #----------------------------------------------------------\n",
        "            fig, ax = plt.subplots(figsize=(20, 20))\n",
        "            ax.imshow(img)\n",
        "            for j in range(len(gt)):\n",
        "                rect1 = patches.Rectangle((int(gt[j][0]),int(gt[j][1])),abs(gt[j][0]-gt[j][2]),\n",
        "                                abs(gt[j][1]-gt[j][3]),linewidth=2,edgecolor='g',facecolor='none')\n",
        "                ax.add_patch(rect1)\n",
        "            for i in range(len(bxs)):\n",
        "                rect = patches.Rectangle((int(bxs[i][0]),int(bxs[i][1])),abs(bxs[i][0]-bxs[i][2]),\n",
        "                                         abs(bxs[i][1]-bxs[i][3]),linewidth=1,edgecolor='r',facecolor='none')\n",
        "                ax.add_patch(rect)\n",
        "            fig.savefig(\"/content/drive/MyDrive/output_images/{}.png\".format(cnt), dpi=90, bbox_inches='tight')\n",
        "            plt.clf()\n",
        "            cnt = cnt + 1\n",
        "            evaluate_maps+=metric.compute()['map'].item()\n",
        "            metric.update(out, targets)\n",
        "        print(\"mAP (evaluate): \",evaluate_maps/len(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQSNlkjPz9KR"
      },
      "outputs": [],
      "source": [
        "!rm /content/drive/MyDrive/output_images/*.*\n",
        "evaluate(model, valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82t3tNeZ1uO9"
      },
      "outputs": [],
      "source": [
        "img,gt=valid_dataset.__getitem__(1)\n",
        "fig, ax = plt.subplots(figsize=(20, 20))\n",
        "\n",
        "img=img.permute(1, 2, 0).cpu().numpy()\n",
        "print(img.shape)\n",
        "gt=gt['boxes'].cpu().numpy()\n",
        "w_img,h_img=img.shape[0],img.shape[1]\n",
        "ax.imshow(img)\n",
        "window_name = 'Image'\n",
        "for j in range(len(gt)):\n",
        "    yolo_bbox1 = (gt[j][0], gt[j][1], gt[j][2], gt[j][3])\n",
        "    x1, y1, x2, y2=(gt[j][0], gt[j][1], gt[j][2], gt[j][3])\n",
        "    rect1 = patches.Rectangle((x1,y1),abs(x1-x2),\n",
        "                               abs(y1-y2),linewidth=1,edgecolor='r',facecolor='none')\n",
        "    ax.add_patch(rect1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0172c15cbcfd410b8312f74ae56fa49f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08b86d585d0e4f04af49c761647d24a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3482e6827d374d52bb2c78edf62109ad",
            "placeholder": "​",
            "style": "IPY_MODEL_618b48cbed3545b89c7e4e1a8bf556b7",
            "value": " 160M/160M [00:02&lt;00:00, 101MB/s]"
          }
        },
        "090bd994095641dca6ecad666d59c138": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3482e6827d374d52bb2c78edf62109ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "389dfcb346e146359d9724c907c43e0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7c35cc1a11c4a02bd8540beb1e64deb",
              "IPY_MODEL_4ef9d029058d412d8a60092c02f12b38",
              "IPY_MODEL_08b86d585d0e4f04af49c761647d24a6"
            ],
            "layout": "IPY_MODEL_ebb74caa5ae54fa8b671e11ab389566f"
          }
        },
        "4ef9d029058d412d8a60092c02f12b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0172c15cbcfd410b8312f74ae56fa49f",
            "max": 167502836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_090bd994095641dca6ecad666d59c138",
            "value": 167502836
          }
        },
        "5540a821425b42c684a6b81f40b31665": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "618b48cbed3545b89c7e4e1a8bf556b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7c35cc1a11c4a02bd8540beb1e64deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e254392fc242426ca953e846e9950f60",
            "placeholder": "​",
            "style": "IPY_MODEL_5540a821425b42c684a6b81f40b31665",
            "value": "100%"
          }
        },
        "e254392fc242426ca953e846e9950f60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebb74caa5ae54fa8b671e11ab389566f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}